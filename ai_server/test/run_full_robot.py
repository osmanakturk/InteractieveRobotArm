import sys
import os
import time
import numpy as np
import cv2
import torch
from PIL import Image
from transformers import AutoModelForVision2Seq, AutoProcessor

# xArm SDK yolunu ekle (EÄŸer kurulu deÄŸilse pip install xarm-python-sdk)
from xarm.wrapper import XArmAPI

# --- AYARLAR ---
ROBOT_IP = "192.168.1.30" # <-- Robotun IP adresini buraya yaz!
SCALE_FACTOR = 80          # Modelin kÃ¼Ã§Ã¼k hareketlerini bÃ¼yÃ¼tme katsayÄ±sÄ± (HÄ±z ayarÄ±)
GRIPPER_THRESHOLD = 0.5    # KÄ±skaÃ§ aÃ§ma/kapama eÅŸiÄŸi

def main():
    print("ðŸš€ SÄ°STEM BAÅžLATILIYOR: TAM KONTROL MODU")

    # 1. ROBOT BAÄžLANTISI
    print(f"ðŸ¤– Robot ({ROBOT_IP}) aranÄ±yor...")
    try:
        arm = XArmAPI(ROBOT_IP)
        arm.motion_enable(enable=True)
        arm.set_mode(0)
        arm.set_state(state=0)
        time.sleep(1)
        print("âœ… Robot BaÄŸlandÄ± ve HazÄ±r!")
    except Exception as e:
        print(f"âŒ Robot BaÄŸlantÄ± HatasÄ±: {e}")
        return

    # BaÅŸlangÄ±Ã§ pozisyonuna git (GÃ¼venli bÃ¶lge)
    print("ðŸ“ BaÅŸlangÄ±Ã§ pozisyonuna gidiliyor...")
    arm.set_position(x=300, y=0, z=200, roll=180, pitch=0, yaw=0, speed=50, wait=True)
    
    # KÄ±skaÃ§ (Gripper) HazÄ±rlÄ±ÄŸÄ±
    arm.set_gripper_enable(True)
    arm.set_gripper_speed(1000)
    arm.set_gripper_position(850, wait=True) # AÃ§Ä±k baÅŸla

    # 2. YAPAY ZEKA MODELÄ°
    print("ðŸ§  Yapay Zeka YÃ¼kleniyor...")
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    model_id = "openvla/openvla-7b"
    
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForVision2Seq.from_pretrained(
        model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, trust_remote_code=True
    ).to(device)
    print("âœ… Beyin HazÄ±r!")

    # 3. KAMERA
    print("ðŸ‘ï¸ Kamera (Index 0) aÃ§Ä±lÄ±yor...")
    cap = cv2.VideoCapture(0) # Index 0 olarak gÃ¼ncelledik
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    if not cap.isOpened():
        print("âŒ Kamera aÃ§Ä±lamadÄ±!")
        return

    print("\nâš¡ KONTROL BAÅžLADI! (Ã‡Ä±kÄ±ÅŸ iÃ§in 'q', Acil Durum iÃ§in 'Ctrl+C')")

    try:
        while True:
            # GÃ¶rÃ¼ntÃ¼ Al
            ret, frame = cap.read()
            if not ret: continue

            # AI iÃ§in HazÄ±rla
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image_pil = Image.fromarray(image_rgb)
            prompt = "In: What action should the robot take to pick up the object?\nOut:"
            
            # Tahmin
            inputs = processor(prompt, image_pil, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            inputs["pixel_values"] = inputs["pixel_values"].to(dtype=torch.bfloat16)
            
            with torch.no_grad():
                action = model.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)
            
            # Veriyi Al (7 boyutlu vektÃ¶r: x, y, z, roll, pitch, yaw, gripper)
            # Model Ã§Ä±ktÄ±larÄ± Ã§ok kÃ¼Ã§Ã¼ktÃ¼r (delta), bunlarÄ± bÃ¼yÃ¼tmeliyiz.
            action_raw = action.flatten()
            
            # --- HAREKET MANTIÄžI ---
            # Modelin X'i -> Robotun Y'si (Genellikle kamera aÃ§Ä±sÄ±na gÃ¶re deÄŸiÅŸir, deneyerek bulacaÄŸÄ±z)
            # Modelin Y'si -> Robotun X'i
            
            # Basit Mapping (Kamera robotun tam karÅŸÄ±sÄ±ndaysa):
            # Model  ->  Robot
            # Ä°leri (Y) -> Ä°leri (X)
            # SaÄŸ (X)   -> SaÄŸ (Y) (veya tersi)
            # YukarÄ± (Z)-> YukarÄ± (Z)
            
            delta_x = action_raw[0] * SCALE_FACTOR
            delta_y = action_raw[1] * SCALE_FACTOR
            delta_z = action_raw[2] * SCALE_FACTOR
            
            # Mevcut pozisyonu al
            # [x, y, z, roll, pitch, yaw]
            code, current_pose = arm.get_position(is_radian=False)
            if code != 0: continue

            # Yeni hedefleri hesapla
            target_x = current_pose[0] + delta_x
            target_y = current_pose[1] + delta_y # EÄŸer ters gidiyorsa burayÄ± (-) yap
            target_z = current_pose[2] + delta_z

            # GÃœVENLÄ°K SINIRLARI (Masa Ã§arpmasÄ±n)
            if target_z < 100: target_z = 100 # Masaya 10cm'den fazla yaklaÅŸma

            # Robota GÃ¶nder (Wait=False ile akÄ±cÄ± hareket)
            # Roll, Pitch, Yaw sabit tutuluyor (180, 0, 0) - Sadece XYZ hareketi
            arm.set_position(x=target_x, y=target_y, z=target_z, roll=180, pitch=0, yaw=0, speed=100, wait=False)

            # Gripper KontrolÃ¼
            gripper_val = action_raw[6]
            if gripper_val > GRIPPER_THRESHOLD:
                 # AÃ§ (850)
                 arm.set_gripper_position(850, wait=False)
                 grip_status = "ACIK"
            else:
                 # Kapa (0)
                 arm.set_gripper_position(0, wait=False)
                 grip_status = "KAPALI"

            # Ekrana Bilgi Yaz
            cv2.putText(frame, f"Move: X{delta_x:.1f} Y{delta_y:.1f} Z{delta_z:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, f"Grip: {grip_status}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            cv2.imshow('Robot Kontrol Ekrani', frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except KeyboardInterrupt:
        print("\nðŸ›‘ ACÄ°L DURDURMA!")
        arm.set_state(4) # Stop
        arm.disconnect()
    except Exception as e:
        print(f"Hata: {e}")
    finally:
        arm.disconnect()
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()