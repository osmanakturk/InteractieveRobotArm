import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import numpy as np
import cv2
import os
import time

# Gereksiz uyarÄ±larÄ± kapat
os.environ["USE_TF"] = "NO"
os.environ["USE_JAX"] = "NO"

def start_robot_intelligence():
    model_id = "openvla/openvla-7b"
    # M1 Mac iÃ§in MPS cihazÄ±
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    
    print(f"ğŸ§  Model MPS ({device}) Ã¼zerinde baÅŸlatÄ±lÄ±yor...")
    
    # 1. Model ve Processor YÃ¼kleme
    try:
        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForVision2Seq.from_pretrained(
            model_id, 
            torch_dtype=torch.bfloat16, 
            low_cpu_mem_usage=True, 
            trust_remote_code=True
        ).to(device)
        print("âœ… Model BaÅŸarÄ±yla YÃ¼klendi!")
    except Exception as e:
        print(f"âŒ Model YÃ¼kleme HatasÄ±: {e}")
        return

    # 2. Kamera HazÄ±rlÄ±ÄŸÄ± (Standart Mod)
    print("ğŸ‘ï¸ Kamera baÅŸlatÄ±lÄ±yor...")
    # RealSense genellikle 1. indekstir. Olmazsa 0 veya 2 dene.
    cap = cv2.VideoCapture(0) # <-- GÃ¶rÃ¼ntÃ¼ gelmezse burayÄ± 0 yap
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    if not cap.isOpened():
        print("âš ï¸ Kamera 1 aÃ§Ä±lamadÄ±, 0 deneniyor...")
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ HiÃ§bir kamera bulunamadÄ±!")
            return

    print("ğŸš€ SÄ°STEM HAZIR! Robot dÃ¼ÅŸÃ¼nÃ¼yor...")

    try:
        while True:
            # Kameradan kare al
            ret, frame = cap.read()
            if not ret:
                print("âš ï¸ Kare atlandÄ±")
                time.sleep(0.1)
                continue

            # OpenCV (BGR) -> PIL (RGB) DÃ¶nÃ¼ÅŸÃ¼mÃ¼
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image_pil = Image.fromarray(image_rgb)
            
            # --- ROBOT BEYNÄ° (OpenVLA) ---
            prompt = "In: What action should the robot take to pick up the object?\nOut:"
            
            # HATAYI Ã‡Ã–ZEN KISIM BURASI:
            # Girdileri oluÅŸturuyoruz ama hemen 'to(dtype)' yapmÄ±yoruz
            inputs = processor(prompt, image_pil, return_tensors="pt")
            
            # Girdileri cihaza doÄŸru formatta taÅŸÄ±yoruz
            inputs = inputs.to(device)
            # Sadece resim verisini bfloat16 yapÄ±yoruz (Metinler int kalmalÄ±)
            inputs["pixel_values"] = inputs["pixel_values"].to(dtype=torch.bfloat16)

            # Tahmin
            with torch.no_grad():
                action = model.predict_action(inputs, unnorm_key="bridge_orig", do_sample=False)
            
            # SonuÃ§larÄ± YazdÄ±r
            # Tensor -> Numpy dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (CPU'ya Ã§ekerek)
            action_raw = action
            
            # Terminal Ã‡Ä±ktÄ±sÄ±
            print(f"ğŸ¤– Action: {np.round(action_raw, 3)}")
            
            # Ekranda GÃ¶ster
            cv2.putText(frame, "AI ANALIZ EDIYOR...", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Aksiyon deÄŸerlerini ekrana da yazalÄ±m
            action_str = str(np.round(action_raw[0][:3], 2)) # Sadece ilk 3 (XYZ) koordinatÄ±
            cv2.putText(frame, f"Move: {action_str}", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
            cv2.imshow('Robot Gozu (OpenVLA)', frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except KeyboardInterrupt:
        print("ğŸ›‘ KullanÄ±cÄ± durdurdu.")
    except Exception as e:
        print(f"âŒ Kritik Hata: {e}")
    finally:
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    start_robot_intelligence()